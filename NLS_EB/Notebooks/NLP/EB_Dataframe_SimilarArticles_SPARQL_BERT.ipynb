{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring Terms in the Encyclopaedia Britannica\n",
    "\n",
    "## Similar terms within an edition - BERT - Transformers\n",
    "\n",
    "\n",
    "https://theaidigest.in/how-to-do-semantic-document-similarity-using-bert/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import collections\n",
    "import matplotlib as mpl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_document(uri):\n",
    "    uri=\"<\"+uri+\">\"\n",
    "    sparql = SPARQLWrapper(\"http://35.228.63.82:3030/eb1/sparql\")\n",
    "    query=\"\"\"\n",
    "    PREFIX eb: <https://w3id.org/eb#>\n",
    "    PREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#>\n",
    "    SELECT ?definition ?term\n",
    "        WHERE {{\n",
    "            %s a eb:Article ;\n",
    "               eb:name ?term ;\n",
    "               eb:definition ?definition . \n",
    "            }\n",
    "            UNION {\n",
    "            %s a eb:Topic ;\n",
    "              eb:name ?term ; \n",
    "              eb:definition ?definition . \n",
    "            }\n",
    "       } \n",
    "    \"\"\" %(uri, uri)\n",
    "    sparql.setQuery(query)\n",
    "    sparql.setReturnFormat(JSON)\n",
    "    results = sparql.query().convert()\n",
    "    term = results[\"results\"][\"bindings\"][0][\"term\"][\"value\"]\n",
    "    definition=results[\"results\"][\"bindings\"][0][\"definition\"][\"value\"]\n",
    "    return term, definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We have dataframe with these columns\n",
    "\n",
    "- definition:           Definition of a term\n",
    "- editionNum:           1,2,3,4,5,6,7,8\n",
    "- editionTitle:         Title of the edition\n",
    "- header:               Header of the page's term                                  \n",
    "- place:                Place where the volume was edited (e.g. Edinburgh)                                    \n",
    "- relatedTerms:         Related terms (see X article)  \n",
    "- altoXML:              File Path of the XML file from which the term belongs       \n",
    "- term:                 Term name                            \n",
    "- positionPage:         Position of ther term in the page     \n",
    "- startsAt:             Number page in which the term definition starts \n",
    "- endsAt:               Number page in which the term definition ends \n",
    "- volumeTitle:          Title of the Volume\n",
    "- typeTerm:             Type of term [Topic| Articles]                                       \n",
    "- year:                 Year of the edition\n",
    "- volumeNum:            Volume number (e.g. 1)\n",
    "- letters:              leters of the volume (A-B)\n",
    "- part:                 Part of the volume (e.g 1)\n",
    "- supplement:           Supplement's Title\n",
    "- supplementsTo:        It suppelements to editions [1, 2, 3....]\n",
    "- numberOfWords:        Number of words per term definition\n",
    "- numberOfTerms:        Number of terms per page\n",
    "- numberOfPages:        Number of pages per volume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import rdflib\n",
    "from rdflib.extras.external_graph_libs import rdflib_to_networkx_multidigraph\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as pl\n",
    "from rdflib import Graph, ConjunctiveGraph, Namespace, Literal\n",
    "from rdflib.plugins.sparql import prepareQuery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "from SPARQLWrapper import SPARQLWrapper, JSON"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.  Selecting just the 100 first elements of  the first volume of 1771"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparql = SPARQLWrapper(\"http://35.228.63.82:3030/eb1/sparql\")\n",
    "query=\"\"\"\n",
    "PREFIX eb: <https://w3id.org/eb#>\n",
    "PREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#>\n",
    "SELECT ?definition ?uri ?term ?vnum ?year ?enum ?letters ?part\n",
    "        WHERE {{\n",
    "    \t?uri a eb:Article .\n",
    "    \t?uri eb:name ?term .\n",
    "        ?uri eb:definition ?definition . \n",
    "        ?v eb:hasPart ?uri.\n",
    "        ?v eb:number ?vnum.\n",
    "        ?v eb:letters ?letters .\n",
    "        ?e eb:hasPart ?v.\n",
    "        ?e eb:publicationYear ?year.\n",
    "        ?e eb:number ?enum.\n",
    "        OPTIONAL {?v eb:part ?part; }\n",
    "        }\n",
    "  \t\tUNION {\n",
    "    \t?uri a eb:Topic .\n",
    "    \t?uri eb:name ?term . \n",
    "        ?uri eb:definition ?definition .\n",
    "        ?v eb:hasPart ?uri.\n",
    "        ?v eb:number ?vnum.\n",
    "        ?v eb:letters ?letters .\n",
    "        ?e eb:hasPart ?v.\n",
    "        ?e eb:publicationYear ?year.\n",
    "        ?e eb:number ?enum.\n",
    "        OPTIONAL {?v eb:part ?part; }\n",
    "        \n",
    "        }\n",
    "   }\n",
    "\"\"\" \n",
    "sparql.setQuery(query)\n",
    "sparql.setReturnFormat(JSON)\n",
    "results = sparql.query().convert()\n",
    "results = sparql.query().convert()\n",
    "documents=[]\n",
    "terms_info=[]\n",
    "uris=[]\n",
    "for r in results[\"results\"][\"bindings\"]:\n",
    "    documents.append(r[\"definition\"][\"value\"])\n",
    "    uris.append(r[\"uri\"][\"value\"])\n",
    "    if \"part\" in r:\n",
    "        terms_info.append([r[\"term\"][\"value\"], r[\"enum\"][\"value\"], r[\"year\"][\"value\"], r[\"part\"][\"value\"], r[\"vnum\"][\"value\"], r[\"letters\"][\"value\"]])\n",
    "    else:\n",
    "        terms_info.append([r[\"term\"][\"value\"], r[\"enum\"][\"value\"], r[\"year\"][\"value\"], \"\" , r[\"vnum\"][\"value\"], r[\"letters\"][\"value\"]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "18117"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(uris)\n",
    "len(terms_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('documents_1ed.txt', 'wb') as fp:\n",
    "    pickle.dump(documents, fp)\n",
    "    \n",
    "with open('terms_info_1ed.txt', 'wb') as fp2:\n",
    "    pickle.dump(terms_info, fp2)\n",
    "    \n",
    "with open('uris_1ed.txt', 'wb') as fp3:\n",
    "    pickle.dump(uris, fp3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1 Train Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "Downloading (…)2f64d/.gitattributes:   0%|          | 0.00/1.18k [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b7a4fb2222fa477eae698a9fdc20b253"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Downloading (…)7edf02f64d/README.md:   0%|          | 0.00/28.0 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "7f907007356a4bf0a1dcfa2a79417dec"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Downloading (…)df02f64d/config.json:   0%|          | 0.00/481 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a8976446c7f64305a58a3ac55c337364"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Downloading pytorch_model.bin:   0%|          | 0.00/439M [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "017f7a4890964c85946eea254cc9ed82"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Downloading (…)7edf02f64d/vocab.txt:   0%|          | 0.00/227k [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e66f34e63d40464683ef558850828d42"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No sentence-transformers model found with name /Users/ly40/.cache/torch/sentence_transformers/emanjavacas_MacBERTh. Creating a new one with MEAN pooling.\n",
      "Some weights of the model checkpoint at /Users/ly40/.cache/torch/sentence_transformers/emanjavacas_MacBERTh were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.decoder.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model = SentenceTransformer('emanjavacas/MacBERTh')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "Batches:   0%|          | 0/2265 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ef635535e1d04aa88f52aabeee43c4f2"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "text_embeddings = model.encode(documents, batch_size = 8, show_progress_bar = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "(18117, 768)"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(text_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_embeddings_1ed = np.array(text_embeddings)\n",
    "np.save('embeddings_1ed.npy', all_embeddings_1ed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "Batches:   0%|          | 0/1 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "5f1e458feacd4d23b2b4251ed33f0514"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "uri=\"https://w3id.org/eb/i/Article/9929192893804340_144850368_PAISLEY_0\"\n",
    "term, definition=get_document(uri)\n",
    "definition_embedding= model.encode(definition, batch_size = 8, show_progress_bar = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity_def=cosine_similarity(\n",
    "    [definition_embedding],\n",
    "    text_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pairwise dense output:\n",
      " [[0.9999994  0.87249523 0.84896547 ... 0.813816   0.84602416 0.8092604 ]\n",
      " [0.87249523 1.0000001  0.9066804  ... 0.8861644  0.9074524  0.86981225]\n",
      " [0.84896547 0.9066804  0.9999995  ... 0.86914456 0.88573086 0.8514845 ]\n",
      " ...\n",
      " [0.813816   0.8861644  0.86914456 ... 1.         0.94909406 0.9389284 ]\n",
      " [0.84602416 0.9074524  0.88573086 ... 0.94909406 1.0000007  0.9250872 ]\n",
      " [0.8092604  0.86981225 0.8514845  ... 0.9389284  0.9250872  0.99999976]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "similarities = cosine_similarity(text_embeddings)\n",
    "print('pairwise dense output:\\n {}\\n'.format(similarities))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "array([[16146,  9183, 12086, ..., 16384,  7527,     0],\n       [12086,  9183,  3011, ...,  5019,  7302,     1],\n       [ 3011,  9183, 12086, ...,  9304, 17531,     2],\n       ...,\n       [16146,  9183, 11428, ...,  9964,  7100, 18114],\n       [ 9183, 11428, 16146, ...,  9619, 18032, 18115],\n       [16146,  3011, 12086, ...,  8142, 18047, 18116]])"
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similarities_sorted = similarities.argsort()\n",
    "similarities_sorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[19], line 9\u001B[0m\n\u001B[1;32m      7\u001B[0m     id_2\u001B[38;5;241m.\u001B[39mappend(array[\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m2\u001B[39m])\n\u001B[1;32m      8\u001B[0m     score\u001B[38;5;241m.\u001B[39mappend(similarities[index][array[\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m2\u001B[39m]])\n\u001B[0;32m----> 9\u001B[0m index_df \u001B[38;5;241m=\u001B[39m \u001B[43mpd\u001B[49m\u001B[38;5;241m.\u001B[39mDataFrame({\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mid_1\u001B[39m\u001B[38;5;124m'\u001B[39m : id_1,\n\u001B[1;32m     10\u001B[0m                           \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mid_2\u001B[39m\u001B[38;5;124m'\u001B[39m : id_2,\n\u001B[1;32m     11\u001B[0m                           \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mscore\u001B[39m\u001B[38;5;124m'\u001B[39m : score})\n\u001B[1;32m     12\u001B[0m \u001B[38;5;28mprint\u001B[39m(p)\n",
      "\u001B[0;31mNameError\u001B[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "id_1 = []\n",
    "id_2 = []\n",
    "score = []\n",
    "for index,array in enumerate(similarities_sorted):\n",
    "    p=len(array)\n",
    "    id_1.append(index)\n",
    "    id_2.append(array[-2])\n",
    "    score.append(similarities[index][array[-2]])\n",
    "index_df = pd.DataFrame({'id_1' : id_1,\n",
    "                          'id_2' : id_2,\n",
    "                          'score' : score})\n",
    "print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents[13278]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
