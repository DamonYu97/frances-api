{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merging EB terms-  NLS -  Encyclopaedia Britannica\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "import numpy as np\n",
    "import collections\n",
    "import string\n",
    "import copy\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from yaml import safe_load\n",
    "from pandas.io.json import json_normalize\n",
    "from difflib import SequenceMatcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_query_results(filename):\n",
    "    with open('../../results_NLS/'+filename, 'r') as f:\n",
    "        query_results = safe_load(f)\n",
    "    return query_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_query_results(filename, results):\n",
    "    with open('../../results_NLS/'+filename, 'w') as f:\n",
    "        documents = yaml.dump(results, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_query_results(query_results):\n",
    "    new_results={}\n",
    "    for edition in query_results:\n",
    "        new_results[edition]=[]\n",
    "        page_list=[]\n",
    "        for page_idx in range(0, len(query_results[edition])):\n",
    "            page_num = query_results[edition][page_idx][0]\n",
    "            if not page_list:\n",
    "                page_list.append(page_num)\n",
    "                new_results[edition].append(query_results[edition][page_idx])\n",
    "            \n",
    "            else:\n",
    "                last_element=page_list[-1]\n",
    "                if page_num >= last_element:\n",
    "                    page_list.append(page_num)\n",
    "                    new_results[edition].append(query_results[edition][page_idx])\n",
    "                else:\n",
    "                    ## insert the new page in page_list\n",
    "                    i_dx=0\n",
    "                    while page_list[i_dx] < page_num:\n",
    "                        i_dx+=1\n",
    "                    page_list.insert(i_dx, page_num)\n",
    "                    new_results[edition].insert(i_dx,query_results[edition][page_idx])\n",
    "    return new_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def consistency_query_results(query_results):\n",
    "      for i in query_results:\n",
    "        for j in range(0, len(query_results[i])):\n",
    "            page_num = query_results[i][j][0]\n",
    "            if j < (len(query_results[i])-1):\n",
    "                next_page_num = query_results[i][j+1][0]\n",
    "                if page_num > next_page_num:\n",
    "                    print(\"INCONSISTENCY for %s: %s and %s\"% (i, page_num, next_page_num))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataframe(query_results):\n",
    "\n",
    "    for edition in query_results:\n",
    "        for page in query_results[edition]:\n",
    "            #print(page[1].keys())\n",
    "            column_list=list(page[1].keys())\n",
    "            break\n",
    "        break\n",
    "        \n",
    "    data=[]\n",
    "    for edition in query_results:\n",
    "        for page in query_results[edition]:\n",
    "            try:\n",
    "                data.append(page[1])\n",
    "               \n",
    "            except:\n",
    "                pass\n",
    "    df = pd.DataFrame(data, columns = column_list)\n",
    "    #removing the columns that I dont need \n",
    "    df= df.drop(['last_term_in_page', 'model', 'num_page_words', 'text_unit', 'type_archive'], axis=1)\n",
    "    #renaming the page num\n",
    "    df= df.rename(columns={\"text_unit_id\": \"startsAt\", \"end_page\":\"endsAt\",\\\n",
    "                           \"type_page\": \"typeTerm\", \"edition\":\"editionTitle\",\\\n",
    "                           \"title\":\"volumeTitle\", \"related_terms\":\"relatedTerms\",\\\n",
    "                           \"source_text_file\": \"altoXML\", \"num_articles\": \"numberOfTerms\", \"num_text_unit\": \"numberOfPages\", \\\n",
    "                           \"num_article_words\":\"numberOfWords\", \"term_id_in_page\":\"positionPage\"})\n",
    "     \n",
    "    #removing 'Page' from the string\n",
    "    df[\"startsAt\"] = df[\"startsAt\"].str.replace(\"Page\", \"\", regex=True)\n",
    "    df[\"startsAt\"] = df[\"startsAt\"].astype(int)\n",
    "    df[\"endsAt\"] = df[\"endsAt\"].astype(int)\n",
    "   \n",
    "    \n",
    "    df['term'] = df[\"term\"].str.replace(\"_def\", \"\",  regex=True)\n",
    "    df['term']= df[\"term\"].str.replace('[^a-zA-Z0-9]', '',  regex=True)\n",
    "    \n",
    "    #mask=df[\"term\"].str.isalpha()\n",
    "    #df_new=df.loc[mask]\n",
    "    \n",
    "    \n",
    "    \n",
    "    df['term'] = df['term'].str.upper()\n",
    "    \n",
    "    \n",
    "    df[\"volumeNum\"] = 0\n",
    "    df[\"letters\"] = \"\"\n",
    "    df[\"part\"] = 0\n",
    "    \n",
    "    \n",
    "    mask = df[\"editionTitle\"].str.contains('Volume')\n",
    "    for i in range(0, len (mask)):\n",
    "     \n",
    "        if mask[i]:\n",
    "            tmp=df.loc[i,'editionTitle'].split(\"Volume \")[1].split(\",\")\n",
    "            if len(tmp)>=1:\n",
    "                volume= tmp[0]\n",
    "                letters = tmp[-1].replace(\" \",\"\")\n",
    "                part_tmp = volume.split(\"Part \")\n",
    "                if len(part_tmp)>1:\n",
    "                    volume=part_tmp[0]\n",
    "                    part = part_tmp[1]\n",
    "    \n",
    "                    try:\n",
    "                        part = int(part)\n",
    "                    except:\n",
    "                        if \"I\" in part:\n",
    "                            part = 1\n",
    "                else:\n",
    "                    part=0\n",
    "\n",
    "                volume = int(volume)\n",
    "                df.loc[i, \"letters\"] = letters\n",
    "                df.loc[i,\"part\"] = part\n",
    "                df.loc[i ,\"volumeNum\"] = volume\n",
    "    \n",
    "    df[\"editionNum\"] = \"0\"\n",
    "    list_editions={\"1\":[\"first\", \"First\"], \"2\":[\"second\", \"Second\"],\"3\":[\"third\", \"Third\"],\n",
    "                   \"4\":[\"fourth\", \"Fourth\"], \"5\":[\"fifth\",\"Fifth\"], \"6\":[\"sixth\",\"Sixth\"], \n",
    "                   \"7\":[\"seventh\", \"Seventh\"], \"8\":[\"eighth\", \"Eighth\"]}\n",
    "    \n",
    "    for ed in list_editions:\n",
    "        for ed_versions in list_editions[ed]:\n",
    "            mask = df[\"editionTitle\"].str.contains(ed_versions)\n",
    "            df.loc[mask, 'editionNum'] = ed\n",
    "            \n",
    "            \n",
    "    df['editionNum']=df[\"editionNum\"].astype(int)    \n",
    "    df[\"supplementTitle\"]=\"\"\n",
    "    df[\"supplementsTo\"]=\"\"\n",
    "    \n",
    "    mask= df[\"volumeTitle\"].str.contains(\"Supplement\")\n",
    "    for i in range(0, len (mask)):\n",
    "        if mask[i]:\n",
    "            df.loc[i, 'supplementTitle'] = df.loc[i, 'volumeTitle']\n",
    "            df.loc[i, 'volumeTitle'] = df.loc[i, 'volumeTitle'] + \",\"+df.loc[i, 'editionTitle']\n",
    "            title= df.loc[i, 'supplementTitle']\n",
    "            related_editions=[]\n",
    "            for ed in list_editions:\n",
    "                for ed_versions in list_editions[ed]:\n",
    "                    if ed_versions in title:\n",
    "                        related_editions.append(ed)\n",
    "                        \n",
    "            df.loc[i, \"supplementsTo\"]=','.join(related_editions)\n",
    "            \n",
    "    df[\"supplementsTo\"] = df.supplementsTo.str.split(\",\").tolist()\n",
    "    a=df[\"archive_filename\"].str.split(\"/\").str[-1]\n",
    "    df['altoXML']= a+ \"/\" + df[\"altoXML\"]   \n",
    "    df= df.drop(['archive_filename'], axis=1)\n",
    "   \n",
    "    df = df[[\"term\", \"definition\", \"relatedTerms\", \"header\", \"startsAt\", \"endsAt\", \"numberOfTerms\",\"numberOfWords\", \"numberOfPages\", \\\n",
    "             \"positionPage\", \"typeTerm\", \"editionTitle\", \"editionNum\", \"supplementTitle\", \"supplementsTo\", \\\n",
    "             \"year\", \"place\", \"volumeTitle\", \"volumeNum\", \"letters\", \"part\", \"altoXML\"]]\n",
    "    \n",
    "    df = df[df['term'] != '']\n",
    "    mask=df[\"term\"].str.isalpha()\n",
    "    df=df.loc[mask] \n",
    "    \n",
    "    ### NEW DECISION: Move Mix Articles as Article\n",
    "    mask = df[\"typeTerm\"].str.contains(\"Mix\")\n",
    "    df.loc[mask, 'typeTerm'] = \"Article\"\n",
    "    ###########\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def similar(a, b):\n",
    "    a=a.lower()\n",
    "    b=b.lower()\n",
    "    return SequenceMatcher(None, a, b).ratio()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def most_frequent_simple(List):\n",
    "    ### removed '' and ' ' keys\n",
    "    if '' in List: \n",
    "        List.remove('')\n",
    "    if len(List) > 1:\n",
    "        a= max(set(List), key = List.count)\n",
    "        return a\n",
    "    else:\n",
    "        return ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def most_frequent(words_list, prev_car=None):\n",
    "    \n",
    "    result=''\n",
    "    if '' in words_list: \n",
    "        words_list.remove('')\n",
    "    \n",
    "    for i in words_list:\n",
    "        if len(i)<=2:\n",
    "            words_list.remove(i)\n",
    "        \n",
    "    if prev_car==\"\":\n",
    "        prev_car= None\n",
    "        \n",
    "    if len(words_list) > 1:\n",
    "        \n",
    "        c = [item for item in Counter(words_list).most_common(2)]\n",
    "        \n",
    "        if len(c) > 1:\n",
    "            if len(c[0][0]) < 1 and len(c[1][0])>=1:\n",
    "                result= c[1][0]\n",
    "        \n",
    "            elif len(c[0][0]) >= 1 and len(c[1][0])<1:\n",
    "                result= c[0][0]\n",
    "\n",
    "            elif len(c[0][0]) < 1 and len(c[1][0])<1:\n",
    "                result=''\n",
    "            \n",
    "            elif c[0][1] == c[1][1]:\n",
    "        \n",
    "                similar_count={}\n",
    "                similar_count[c[0][0]]=0\n",
    "                similar_count[c[1][0]]=0\n",
    "        \n",
    "                for i in words_list:\n",
    "                    if i != c[0][0] and i!=c[1][0]:\n",
    "                        \n",
    "                \n",
    "                        if similar(i, c[0][0]) > similar(i, c[1][0]):\n",
    "                            similar_count[c[0][0]]+=1\n",
    "                    \n",
    "                        elif similar(i, c[0][0]) < similar(i, c[1][0]):\n",
    "                            similar_count[c[1][0]]+=1\n",
    "                 \n",
    "                if  similar_count[c[0][0]] > similar_count[c[1][0]]:\n",
    "                    result= c[0][0]\n",
    "                \n",
    "                elif similar_count[c[0][0]] < similar_count[c[1][0]]:\n",
    "        \n",
    "                    result= c[1][0]\n",
    "            \n",
    "            \n",
    "                if prev_car:\n",
    "                    if c[0][0][0] == prev_car:\n",
    "                    \n",
    "                        result= c[0][0]\n",
    "                        \n",
    "                    elif c[1][0][0] == prev_car:\n",
    "                     \n",
    "                        result= c[1][0]\n",
    "                    \n",
    "                    elif c[0][0][0]> prev_car:\n",
    "                        result= c[0][0]\n",
    "                    \n",
    "                    else:\n",
    "                        result= c[1][0]\n",
    "                     \n",
    "                \n",
    "                else:\n",
    "                    result= c[0][0]\n",
    "            else:\n",
    "                 result= c[0][0]\n",
    "        else:\n",
    "            result= c[0][0]\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_string(term, List):\n",
    "    flag = 0\n",
    "    for element in List:\n",
    "        if term in element:\n",
    "            flag = 1\n",
    "            break\n",
    "    if flag == 1:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_topics_terms(term):\n",
    "    table = str.maketrans('', '', string.ascii_lowercase)\n",
    "    return term.translate(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataframe_from_file(filename):\n",
    "    with open('../../results_NLS/'+filename, 'r') as f:\n",
    "        query_results = safe_load(f)\n",
    "    \n",
    "    df = create_dataframe(query_results)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prune_json(json_dict):\n",
    "    \"\"\"\n",
    "    Method that given a JSON object, removes all its empty fields.\n",
    "    This method simplifies the resultant JSON.\n",
    "    :param json_dict input JSON file to prune\n",
    "    :return JSON file removing empty values\n",
    "    \"\"\"\n",
    "    final_dict = {}\n",
    "    if not (isinstance(json_dict, dict)):\n",
    "        # Ensure the element provided is a dict\n",
    "        return json_dict\n",
    "    else:\n",
    "        for a, b in json_dict.items():\n",
    "            if b or isinstance(b, bool):\n",
    "                if isinstance(b, dict):\n",
    "                    aux_dict = prune_json(b)\n",
    "                    if aux_dict:  # Remove empty dicts\n",
    "                        final_dict[a] = aux_dict\n",
    "                elif isinstance(b, list):\n",
    "                    aux_list = list(filter(None, [prune_json(i) for i in b]))\n",
    "                    if len(aux_list) > 0:  # Remove empty lists\n",
    "                        final_dict[a] = aux_list\n",
    "                else:\n",
    "                    final_dict[a] = b\n",
    "    return final_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_entries(query_results_updated, eliminate_pages):\n",
    "    new_results={}\n",
    "    for edition in query_results_updated:\n",
    "        new_results[edition]=[]\n",
    "        for page_idx in range(0, len(query_results_updated[edition])):\n",
    "            if page_idx not in eliminate_pages[edition]:\n",
    "                new_results[edition].append(query_results_updated[edition][page_idx])\n",
    "    return new_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deleting_adding_entries(query_results_up, eliminate_pages, create_entries):\n",
    "    new_results={}\n",
    "    flag = 1\n",
    "    for edition in query_results_up:\n",
    "        new_results[edition]=[]\n",
    "        for page_idx in range(0, len(query_results_up[edition])):\n",
    "            if page_idx not in eliminate_pages[edition]:\n",
    "                new_results[edition].append(query_results_up[edition][page_idx])\n",
    "            else:\n",
    "                for new_pages in create_entries[edition][page_idx]:\n",
    "                    new_results[edition].append(new_pages)\n",
    "            \n",
    "        \n",
    "    return new_results      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def related_terms_info(related_terms):\n",
    "    related_data=[]\n",
    "    for elem in related_terms:\n",
    "        if elem.isupper() or \".\" in elem or \",\" in elem:\n",
    "            elem=elem.split(\".\")[0]\n",
    "            term=elem.split(\",\")[0]\n",
    "            if len(term)>2 and term[0].isupper() :\n",
    "                m = re.search('^([0-9]+)|([IVXLCM]+)\\\\.?$', term)\n",
    "                if m is None:\n",
    "                    term_up = term.upper()\n",
    "                    if term_up !=\"FIG\" and term_up !=\"NUMBER\" and term_up!=\"EXAMPLE\" and term_up!=\"PLATE\" and term_up!=\"FIGURE\":\n",
    "                        #related_data.append(term_up) and term_up!=\"EXAMPLE\" and term_up!=\"PLATE\" and term_up!=\"FIGURE\"\n",
    "                        related_data.append(term_up.strip())\n",
    "    return related_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fixing_fullPages(query_results):\n",
    "    parts_string=[\"Part\", \"Fart\", \"Parc\", \"CPart\",\"P\", \"PI\", \"PII\", \"PIII\", \"P\", \"C\", \"PXXIV\", \"IV\", \"VI\" \"VII\", \"VIII\", \"IX\", \"XI\"]\n",
    "    create_entries={}\n",
    "    eliminate_pages={}\n",
    "    for edition in query_results:\n",
    "        create_entries[edition]={}\n",
    "        eliminate_pages[edition]=[]\n",
    "        flag_p = 1\n",
    "        for page_idx in range(0, len(query_results[edition])):\n",
    "            element = query_results[edition][page_idx][1]\n",
    "            current_page = query_results[edition][page_idx][0]\n",
    "            \n",
    "            if int(current_page) <= 10:\n",
    "                element[\"type_page\"] = \"FullPage\"\n",
    "            \n",
    "            elif int(current_page) > 10 and int(current_page) < 20:\n",
    "                if element[\"type_page\"]!=\"FullPage\":\n",
    "                    element = page2full_pages(element)\n",
    "                    next_element= query_results[edition][page_idx+1][1]\n",
    "                    \n",
    "                if element[\"type_page\"]!=\"FullPage\" and next_element[\"type_page\"]==\"FullPage\" and len(element[\"definition\"])<50:\n",
    "\n",
    "                    element[\"type_page\"] = \"FullPage\"\n",
    "                \n",
    "            elif int(current_page)>=20 and element[\"type_page\"] == \"Empty\" and (len(element[\"term\"])<=5 or check_string(element[\"term\"], parts_string)) and element[\"num_articles\"]< 5 and len(element[\"definition\"])<100:\n",
    "                element[\"type_page\"] = \"FullPage\"\n",
    "                #print(\"changing-1 to FULL PAGE %s\" %current_page)\n",
    "            \n",
    "            elif int(current_page)>=20 and element[\"type_page\"] == \"Topic\" and (len(element[\"term\"])<=5 or check_string(element[\"term\"], parts_string)) and len(element[\"definition\"])<100:\n",
    "                element[\"type_page\"] = \"FullPage\"\n",
    "                #print(\"changing-2 to FULL PAGE %s\" %current_page)\n",
    "                \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fixing_articles_1st(query_results):\n",
    "    create_entries={}\n",
    "    eliminate_pages={}\n",
    "    parts_string=[\"S\", \"HXXA\", \"P\", \"C\",\"\", \" \", \"_\", \"-\", \"Part\", \"Fart\", \"Parc\", \"CPart\",\"P\", \"PI\", \"PII\", \"PIII\", \"P\", \"C\", \"IV\", \"VI\" \"VII\", \"VIII\", \"IX\", \"XI\"]\n",
    "    for edition in query_results:\n",
    "        create_entries[edition]={}\n",
    "        eliminate_pages[edition]=[]\n",
    "        flag_p = 1\n",
    "        for page_idx in range(0, len(query_results[edition])):\n",
    "            element = query_results[edition][page_idx][1]\n",
    "            element_page = query_results[edition][page_idx][0]\n",
    "            flag = 0\n",
    "            \n",
    "            if (element[\"type_page\"]==\"Empty\" and element[\"num_articles\"]>2):\n",
    "                \n",
    "                print(\"Moving %s which is empty to Articles\" %element_page)\n",
    "                element[\"type_page\"]=\"Article\"\n",
    "            \n",
    "            #if (element[\"type_page\"]==\"Topic\" and len(element[\"term\"])<=8) or (element[\"type_page\"]==\"FullPage\" and len(element[\"term\"])<=8):\n",
    "            if ((element[\"type_page\"]==\"Topic\" and len(element[\"term\"])<=8)) or (element[\"type_page\"]==\"FullPage\" and element_page>15) or (element[\"type_page\"]==\"Empty\" and element[\"num_articles\"]==1):\n",
    "                #print(\"ENTRO Page %s  - type page %s, term %s\" %(element_page, element[\"type_page\"], element[\"term\"]))\n",
    "                list_terms=[]\n",
    "                new_entries=[]\n",
    "                definition=element[\"definition\"]\n",
    "                definition_list= definition.split(\" \")\n",
    "                term = element[\"term\"].strip()\n",
    "                flag = 0\n",
    "                sub_elements=[]\n",
    "                for word_idx in range(0, len(definition_list)):\n",
    "                    word = definition_list[word_idx]\n",
    "                    if word.isupper() and \",\" in word and len(word)>3 and \"See \"!= definition_list[word_idx-1] and \"SEE \" != definition_list[word_idx-1]:\n",
    "                        sub_elements.append((word.split(\",\")[0],word_idx))\n",
    "                        flag = 1\n",
    "                \n",
    "                \n",
    "                if flag and len(sub_elements) >= 5: \n",
    "                    #print(\"ENTRO-2 Page %s - new def %s\" % (element_page, sub_elements))\n",
    "                    for elem_idx in range(0, len(sub_elements)):\n",
    "                        term_id = 0\n",
    "                       \n",
    "                        new_element={}\n",
    "                        elem=sub_elements[elem_idx]\n",
    "                        new_element[\"term\"]=elem[0].strip()\n",
    "     \n",
    "                        if elem_idx+1 < len(sub_elements):\n",
    "                            sentence=definition_list[elem[1]+1: sub_elements[elem_idx+1][1]]\n",
    "                            new_element[\"definition\"]=' '.join(sentence)\n",
    "                       \n",
    "                            \n",
    "                        else:\n",
    "                            new_element[\"last_term_in_page\"] = 1\n",
    "                            try:\n",
    "                                sentence= definition_list[elem[1]+1:][1]\n",
    "                                new_element[\"definition\"]=' '.join(sentence)\n",
    "        \n",
    "                            except:\n",
    "                                sentence= definition_list[elem[1]:]\n",
    "                                if len(sentence) > 3:\n",
    "                                    new_element[\"definition\"]=' '.join(sentence)\n",
    "       \n",
    "                        if \"definition\" in new_element: \n",
    "                            #%and len(new_element[\"term\"])>=3:\n",
    "                            \n",
    "                            new_element[\"type_page\"] = \"Article\" \n",
    "                            new_element[\"num_article_words\"] = len(sentence)  \n",
    "                            #### related terms ##### \n",
    "                            related_terms=[]\n",
    "                            if \"See \" in new_element[\"definition\"]:\n",
    "                                related_terms= new_element[\"definition\"].split(\"See \")[1]\n",
    "                            elif \"SEE \" in new_element[\"definition\"]:\n",
    "                                related_terms= new_element[\"definition\"].split(\"SEE \")[1]  \n",
    "                            new_element[\"related_terms\"]=related_terms_info(related_terms)\n",
    "                            ####\n",
    "                            \n",
    "                            new_element[\"term_id_in_page\"]=term_id \n",
    "                            new_element[\"archive_filename\"]= element[\"archive_filename\"]\n",
    "                            new_element[\"header\"] = element[\"header\"]\n",
    "                            new_element[\"model\"] = element[\"model\"]\n",
    "                            new_element[\"num_page_words\"]= element[\"num_page_words\"]\n",
    "                            new_element[\"num_text_unit\"] = element[\"num_text_unit\"]\n",
    "                            new_element[\"place\"] = element[\"place\"]\n",
    "                            new_element[\"source_text_file\"] = element[\"source_text_file\"]\n",
    "                            new_element[\"text_unit\"] = element[\"text_unit\"]\n",
    "                            new_element[\"text_unit_id\"] = element[\"text_unit_id\"]\n",
    "                            new_element[\"title\"] = element[\"title\"]\n",
    "                            new_element[\"type_archive\"] = element[\"type_archive\"]\n",
    "                            new_element[\"year\"] = element[\"year\"]\n",
    "                            new_element[\"end_page\"] =int(element['text_unit_id'].split(\"Page\")[1])\n",
    "                            new_element[\"edition\"] = element[\"edition\"]\n",
    "                            \n",
    "                            new_entries.append(new_element)\n",
    "                            list_terms.append(new_element[\"term\"].strip())\n",
    "                            term_id += 1\n",
    "                \n",
    "                r_removals=[]\n",
    "                l_term = 0\n",
    "                for l_term in range(0, len(list_terms)):\n",
    "                    if check_special_caracters(list_terms[l_term]) or has_numbers(list_terms[l_term]):\n",
    "                        r_removals.append(list_terms[l_term]) \n",
    "                    l_term=l_term +1\n",
    "\n",
    "                for r_term in r_removals:\n",
    "                    list_terms.remove(r_term)\n",
    "                        \n",
    "                if len(list_terms) >= 12:\n",
    "                    if len(list_terms) == 1 and list_terms[0]== element[\"term\"]:\n",
    "                        pass\n",
    "                    elif len(list_terms)<=6 and next_element[\"type_page\"]==\"Topic\" and prev_element[\"type_page\"]==\"Topic\":\n",
    "                        pass\n",
    "                    else:\n",
    "                        for i in new_entries:\n",
    "                            i[\"num_articles\"] = len(list_terms)\n",
    "                        #print(\"MOVING TOPICS TO ARTILCES PAGE %s\" %query_results[edition][page_idx][0])\n",
    "                        eliminate_pages[edition].append(page_idx)\n",
    "                        create_entries[edition][page_idx]=[]\n",
    "                        for new_d in new_entries:\n",
    "                            create_entries[edition][page_idx].append([element_page, new_d])\n",
    "                             \n",
    "   \n",
    "    new_results = deleting_adding_entries(query_results, eliminate_pages, create_entries)\n",
    "    return new_results\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_special_caracters(mystring):\n",
    "    return any(not c.isalnum() for c in mystring)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def has_numbers(mystring):\n",
    "    return any(c.isdigit() for c in mystring)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fixing_topics_1st(query_results):\n",
    "    eliminate_pages={}\n",
    "    for edition in query_results:\n",
    "        eliminate_pages[edition]=[]\n",
    "        page_idx = 0\n",
    "        \n",
    "        while page_idx < len(query_results[edition]):\n",
    "            element = query_results[edition][page_idx][1]\n",
    "            current_page = query_results[edition][page_idx][0]\n",
    "            \n",
    "            if (element[\"type_page\"]==\"Empty\" and element[\"num_articles\"]<2) or ((element[\"num_articles\"] < 3) and ((element[\"type_page\"]==\"Article\") or element[\"type_page\"]==\"Mix\")):\n",
    "            \n",
    "                prev_element = query_results[edition][page_idx-1][1]\n",
    "                \n",
    "                if prev_element[\"type_page\"]==\"Topic\":\n",
    "                    #print(\"ED %s MOVING Page %s - type_page %s term %s to Topic\" %(edition, current_page, element[\"type_page\"],  element[\"term\"]))\n",
    "                    tmp_type = element[\"type_page\"]\n",
    "                    element[\"type_page\"] = \"Topic\"\n",
    "\n",
    "                    if tmp_type ==\"Empty\":\n",
    "                        element[\"term\"] = prev_element[\"term\"]\n",
    "                    \n",
    "                    elif similar(prev_element[\"term\"].strip(), element[\"header\"].strip()) > 0.70 or similar(prev_element[\"term\"].strip(), element[\"term\"].strip()) or prev_element[\"term\"].strip() in element[\"definition\"]:\n",
    "                        element[\"term\"] = prev_element[\"term\"]\n",
    "                    else:\n",
    "                        element[\"term\"] = element[\"header\"].strip()\n",
    "                    \n",
    "                    if element[\"num_articles\"] > 1:\n",
    "                        for i in range(1, element[\"num_articles\"]):\n",
    "                            if page_idx + 1 < len(query_results[edition]):\n",
    "                                page_idx += 1\n",
    "                                n_element = query_results[edition][page_idx][1]\n",
    "                                element[\"definition\"]+=n_element[\"definition\"]\n",
    "                                element[\"num_article_words\"]+=n_element[\"num_article_words\"]\n",
    "                                element[\"related_terms\"]+= n_element[\"related_terms\"]\n",
    "                                eliminate_pages[edition].append(page_idx)\n",
    "                            else:\n",
    "                                print(\"Dont entering here - element %s - term %s -  page %s - page_idx %s - len %s\" %(edition, element[\"term\"], query_results[edition][page_idx][0], page_idx, len(query_results[edition])))\n",
    "                            \n",
    "                    element[\"num_articles\"] = 1    \n",
    "            page_idx +=1   \n",
    "        \n",
    "    new_results= delete_entries(query_results, eliminate_pages)              \n",
    "    return new_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_articles(query_results):\n",
    "    eliminate_pages={}\n",
    "    page_number_dict={}\n",
    "    for edition in query_results:\n",
    "        eliminate_pages[edition]=[]\n",
    "        page_number_dict[edition]={}\n",
    "\n",
    "        for page_idx in range(0, len(query_results[edition])):\n",
    "            prev_number = -1\n",
    "            current_page=query_results[edition][page_idx][0]\n",
    "            \n",
    "            if current_page not in page_number_dict[edition]:\n",
    "                page_number_dict[edition][current_page]=page_idx\n",
    "            \n",
    "            element = query_results[edition][page_idx][1]                               \n",
    "                \n",
    "            \n",
    "            ###########################################        \n",
    "            \n",
    "            if \"previous_page\" in element['term']:\n",
    "                current_definition= element[\"definition\"]\n",
    "                previous_page_idx= page_idx -1\n",
    "                num_article_words=element[\"num_article_words\"]\n",
    "                related_terms=element[\"related_terms\"]\n",
    "            \n",
    "                prev_elements = query_results[edition][previous_page_idx][1]\n",
    "                prev_number = query_results[edition][previous_page_idx][0]\n",
    "                if prev_elements[\"last_term_in_page\"] and current_page > prev_number:\n",
    "                    prev_elements[\"definition\"]+=current_definition\n",
    "                    prev_elements[\"num_article_words\"]+=num_article_words\n",
    "                    prev_elements[\"related_terms\"]+= related_terms\n",
    "                    prev_elements[\"end_page\"] = current_page\n",
    "                    \n",
    "                    if prev_number in page_number_dict[edition] and prev_number != -1:\n",
    "                        for prev_articles_idx in range(page_number_dict[edition][prev_number], page_idx):\n",
    "                       \n",
    "                            if query_results[edition][prev_articles_idx][0] == prev_number:\n",
    "                           \n",
    "                                 query_results[edition][prev_articles_idx][1][\"num_page_words\"]+=num_article_words\n",
    "                    else:\n",
    "                        print(\"Edition %s -ERROR between current page %s  and prev page %s-\" % (edition,current_page, prev_number))\n",
    "                  \n",
    "                    pd_i = page_idx \n",
    "                    for i in range(1, element[\"num_articles\"]):\n",
    "                        if pd_i + 1 < len(query_results[edition]):\n",
    "                            pd_i += 1\n",
    "                            if query_results[edition][pd_i][0] == current_page:\n",
    "                                n_element = query_results[edition][pd_i][1]\n",
    "                                n_element[\"num_page_words\"]-=num_article_words\n",
    "                                n_element[\"num_articles\"]-=1\n",
    "                 \n",
    "    \n",
    "                \n",
    "                eliminate_pages[edition].append(page_idx)\n",
    "            else:\n",
    "                element[\"end_page\"] = current_page  \n",
    "   \n",
    "    new_results= delete_entries(query_results, eliminate_pages)\n",
    "    \n",
    "    return new_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def removing_fullpages(query_results):\n",
    "    \n",
    "    eliminate_pages={}\n",
    "    for edition in query_results:\n",
    "        eliminate_pages[edition]=[]\n",
    "        for page_idx in range(0, len(query_results[edition])):\n",
    "            element=query_results[edition][page_idx][1]\n",
    "            if element[\"type_page\"]==\"FullPage\":\n",
    "                eliminate_pages[edition].append(page_idx)\n",
    "        \n",
    "\n",
    "    new_results= delete_entries(query_results, eliminate_pages)\n",
    "    \n",
    "    return new_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def page2full_pages(element):\n",
    "\n",
    "    \n",
    "    term = element[\"term\"]\n",
    "    header = element[\"header\"]\n",
    "    type_page = element[\"type_page\"]\n",
    "    definition = element[\"definition\"]\n",
    "  \n",
    "   \n",
    "    if (type_page == \"Empty\") and len(definition)<100 and element[\"num_articles\"]<2:\n",
    "        type_page =\"FullPage\"\n",
    "    \n",
    "    elif (\"PREFACE\" in term) or (\"PREFACE\" in header):\n",
    "        term = \"PREFACE\"\n",
    "        header = \"PREFACE\"\n",
    "        type_page=\"FullPage\"\n",
    "       \n",
    "    \n",
    "    elif (\"PLATE\" in term) or (\"PLARF\" in term) or (\"ELATE\" in term) or (\"TLAFE\" in term):\n",
    "        header = \"Plate\"\n",
    "        term = \"Plate\"\n",
    "        type_page = \"FullPage\"\n",
    "      \n",
    "        \n",
    "    elif (\"PLATE\" in header) or (\"PLAFR\" in header) or (\"ELATE\" in header) or (\"TLAFE\" in header):\n",
    "        header = \"Plate\"\n",
    "        term = \"Plate\"\n",
    "        type_page = \"FullPage\"\n",
    "      \n",
    "        \n",
    "    elif (\"ARTSANDSCI\" in term) or (\"ARTSANDSCI\" in header):\n",
    "        header = \"FrontPage\"\n",
    "        term = \"FrontPage\"\n",
    "        type_page=\"FullPage\"\n",
    "        \n",
    "\n",
    "        \n",
    "    elif \"ERRATA\" in term or (\"ERRATA\" in header):\n",
    "        header = \"ERRATA\"\n",
    "        term = \"ERRATA\"\n",
    "        type_page=\"FullPage\"\n",
    "       \n",
    "        \n",
    "   \n",
    "    elif (\" LISTOFAUTHORSC\" in term) or (\"LISTOFAUTHORS\" in term) or (\"LISTOFAUTHORSC\" in term) or (\"LISTAUTHORS\" in term):\n",
    "        header = \"AuthorList\"\n",
    "        term = \"AuthorList\"\n",
    "        type_page=\"FullPage\"\n",
    "        \n",
    "        \n",
    "    elif (\" LISTOFAUTHORSC\" in header) or (\"LISTOFAUTHORS\" in header) or (\"LISTOFAUTHORSC\" in header) or (\"LISTAUTHORS\" in header):\n",
    "        header = \"AuthorList\"\n",
    "        term = \"AuthorList\"\n",
    "        type_page=\"FullPage\"\n",
    "       \n",
    "        \n",
    "    \n",
    "    element[\"term\"] = term\n",
    "    element[\"header\"] = header\n",
    "    element[\"type_page\"] = type_page\n",
    "   \n",
    "    return element\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fixing_fullPages(query_results):\n",
    "    parts_string=[\"Part\", \"Fart\", \"Parc\", \"CPart\",\"P\", \"PI\", \"PII\", \"PIII\", \"P\", \"C\", \"IV\", \"VI\" \"VII\", \"VIII\", \"IX\", \"XI\"]\n",
    "    create_entries={}\n",
    "    eliminate_pages={}\n",
    "    for edition in query_results:\n",
    "        create_entries[edition]={}\n",
    "        eliminate_pages[edition]=[]\n",
    "        flag_p = 1\n",
    "        for page_idx in range(0, len(query_results[edition])):\n",
    "            element = query_results[edition][page_idx][1]\n",
    "            current_page = query_results[edition][page_idx][0]\n",
    "            \n",
    "            if int(current_page) <= 10:\n",
    "                element[\"type_page\"] = \"FullPage\"\n",
    "            \n",
    "            elif int(current_page) > 10 and int(current_page) < 20:\n",
    "                if element[\"type_page\"]!=\"FullPage\":\n",
    "                    element = page2full_pages(element)\n",
    "                    next_element= query_results[edition][page_idx+1][1]\n",
    "                    \n",
    "                if element[\"type_page\"]!=\"FullPage\" and next_element[\"type_page\"]==\"FullPage\" and len(element[\"definition\"])<50:\n",
    "                    print(\"changing-3 to FULL PAGE %s\" %current_page)\n",
    "                    element[\"type_page\"] = \"FullPage\"\n",
    "                \n",
    "            elif int(current_page)>=20 and element[\"type_page\"] == \"Empty\" and (len(element[\"term\"])<=4 or check_string(element[\"term\"], parts_string)) and element[\"num_articles\"]< 2 and len(element[\"definition\"])<50:\n",
    "                element[\"type_page\"] = \"FullPage\"\n",
    "                print(\"changing-1 to FULL PAGE %s\" %current_page)\n",
    "            \n",
    "            elif int(current_page)>=20 and element[\"type_page\"] == \"Topic\" and (len(element[\"term\"])<=4 or check_string(element[\"term\"], parts_string)) and len(element[\"definition\"])<100:\n",
    "                element[\"type_page\"] = \"FullPage\"\n",
    "                print(\"changing-2 to FULL PAGE %s\" %current_page)\n",
    "                \n",
    "            \n",
    "            \n",
    "                \n",
    "                \n",
    "                \n",
    "            \n",
    "    return query_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_topics(query_results):\n",
    "    eliminate_pages={}\n",
    "    provenance_removal={}\n",
    "    freq_topics_terms={}\n",
    "    merged_topics={}\n",
    "    character_terms=[]\n",
    "    parts_string=[\"S\", \"HXXA\", \"P\", \"C\",\"\", \" \", \"_\", \"-\", \"Part\", \"Fart\", \"Parc\", \"CPart\",\"P\", \"PI\", \"PII\", \"PIII\", \"P\", \"C\", \"IV\", \"VI\" \"VII\", \"VIII\", \"IX\", \"XI\"]\n",
    "    for edition in query_results:\n",
    "        eliminate_pages[edition]=[]\n",
    "        provenance_removal[edition]=[]\n",
    "        freq_topics_terms[edition]={}\n",
    "        merged_topics[edition]={}\n",
    "        \n",
    "        page_idx = 0\n",
    "        while page_idx < len(query_results[edition]):\n",
    "            current_page=query_results[edition][page_idx][0]        \n",
    "            element = query_results[edition][page_idx][1]\n",
    "\n",
    "            if \"Topic\" in element['type_page']:\n",
    "                term=element[\"term\"].strip()\n",
    "                while check_string(term, parts_string) and page_idx < len(query_results[edition])-1:\n",
    "    \n",
    "                    page_idx = page_idx + 1\n",
    "                    next_element = query_results[edition][page_idx][1]\n",
    "                    term= next_element[\"term\"].strip()\n",
    "                    element[\"definition\"]+=next_element[\"definition\"]\n",
    "                    element[\"num_article_words\"]+=next_element[\"num_article_words\"]\n",
    "                    element[\"num_page_words\"]+=next_element[\"num_page_words\"]                  \n",
    "                    element[\"related_terms\"]+= next_element[\"related_terms\"]\n",
    "                    element[\"end_page\"] = next_element['end_page']\n",
    "                    provenance_removal[edition].append(element[\"end_page\"])\n",
    "                    eliminate_pages[edition].append(page_idx)\n",
    "                    \n",
    "                \n",
    "                #print(\"STUDYING %s at %s\" %(term, query_results[edition][page_idx][0] ))\n",
    "                clean_term=clean_topics_terms(term)\n",
    "        \n",
    "                p_id= page_idx + 1\n",
    "                flag_force = 0  \n",
    "                if p_id < len(query_results[edition]):\n",
    "                    flag = 0\n",
    "                    \n",
    "                    while p_id < len(query_results[edition]) and flag == 0:\n",
    "                        \n",
    "                        next_element = query_results[edition][p_id][1]\n",
    "                        \n",
    "                        if not check_string(next_element[\"term\"], parts_string):\n",
    "                            next_term=clean_topics_terms(next_element[\"term\"].strip())\n",
    "            \n",
    "                        else:\n",
    "                            next_term=next_element[\"term\"].strip()\n",
    "                            \n",
    "                        if p_id < len(query_results[edition])-1:    \n",
    "                            \n",
    "                            two_next_element = query_results[edition][p_id+1][1]\n",
    "                            if not check_string(two_next_element[\"term\"], parts_string):\n",
    "                                two_next_term=clean_topics_terms(two_next_element[\"term\"])\n",
    "                            else:\n",
    "                                two_next_term=two_next_element[\"term\"].strip()\n",
    "                         \n",
    "                        else:\n",
    "                            two_next_term=\"\"\n",
    "                            \n",
    "                    \n",
    "                                \n",
    "                        #m1 = re.search('^([0-9]+)|([IVXLCM]+)\\\\.?$', next_element[\"term\"])\n",
    "                        #m2 = re.search('^([0-9]+)|([IVXLCM]+)\\\\.?$', two_next_element[\"term\"])\n",
    "                        \n",
    "                        definition1= next_element[\"definition\"]\n",
    "                        \n",
    "                        #print(\"PAGE %s, len def %s\" %(query_results[edition][p_id][0], len(definition)))\n",
    "                    \n",
    "                        if similar(clean_term, next_term) > 0.70 or len(next_term)<=3 or len(two_next_term)<=3 or len(definition1)<=30  or check_string(next_term, parts_string) or check_string(two_next_term, parts_string)  or next_term in clean_term or two_next_term in clean_term or similar(clean_term, two_next_term) > 0.70: \n",
    "                           \n",
    "                            if clean_term!=\"\" or clean_term!=\" \":\n",
    "                                if clean_term not in merged_topics[edition]:\n",
    "                                    merged_topics[edition][clean_term]=[]\n",
    "                                    merged_topics[edition][clean_term].append(clean_term)\n",
    "                        \n",
    "                                if not check_string(next_term, parts_string) and len(next_term)>3:\n",
    "                                     merged_topics[edition][clean_term].append(next_term)\n",
    "                            \n",
    "                         \n",
    "                            element[\"definition\"]+=next_element[\"definition\"]\n",
    "                            element[\"num_article_words\"]+=next_element[\"num_article_words\"]\n",
    "                            element[\"num_page_words\"]+=next_element[\"num_page_words\"]                  \n",
    "                            element[\"related_terms\"]+= next_element[\"related_terms\"]\n",
    "                            element[\"end_page\"] = next_element['end_page']\n",
    "                            provenance_removal[edition].append(element[\"end_page\"])\n",
    "                            eliminate_pages[edition].append(p_id)\n",
    "                            #print(\"SIMILAR-TERMS, clean term %s, next_term %s, similarity %s,  len def1 %s, check1 %s, check 2 % future similarity %s, two_next_term %s\" %(clean_term, next_term, similar(clean_term, next_term), len(definition1), check_string(next_term, parts_string),  check_string(two_next_term, parts_string), similar(clean_term, two_next_term), two_next_term))\n",
    "                            p_id= p_id + 1\n",
    "                            \n",
    "                            if similar(clean_term, two_next_term) > 0.70 or len(two_next_term)<=3  or check_string(two_next_term, parts_string) or two_next_term in clean_term:   \n",
    "                                ## adding the two nexts ones. \n",
    "                                \n",
    "                                element[\"definition\"]+=two_next_element[\"definition\"]\n",
    "                                element[\"num_article_words\"]+=two_next_element[\"num_article_words\"]\n",
    "                                element[\"num_page_words\"]+=two_next_element[\"num_page_words\"]                  \n",
    "                                element[\"related_terms\"]+= two_next_element[\"related_terms\"]\n",
    "                                element[\"end_page\"] = two_next_element['end_page']\n",
    "                                \n",
    "                                provenance_removal[edition].append(element[\"end_page\"])\n",
    "                                \n",
    "                                if not check_string(two_next_term, parts_string) and len(two_next_term)>3:\n",
    "                                     merged_topics[edition][clean_term].append(two_next_term)\n",
    "                                        \n",
    "                                eliminate_pages[edition].append(p_id)\n",
    "                                #print(\"SIMILAR-2TERMS, clean term %s, two_next_terms %s, check %s\" %(clean_term, two_next_term, check_string(two_next_term, parts_string)))\n",
    "                                p_id= p_id + 1\n",
    "\n",
    "                                \n",
    "                        \n",
    "                        else:\n",
    "                            #print(\"PASANDO AL SIGUIENTE: clean_term %s, next_term %s, two_next_terms %s \" %(clean_term, next_term, two_next_term))\n",
    "                            p_id= p_id + 1\n",
    "                            flag = 1\n",
    "                    \n",
    "                    #print(\"FLAG %s, next page is %s\" %(flag, query_results[edition][p_id][0]))\n",
    "        \n",
    "               \n",
    "                page_idx = p_id\n",
    "                if clean_term in merged_topics[edition]:\n",
    "                   \n",
    "                    if character_terms:\n",
    "                        freq_term=most_frequent(merged_topics[edition][clean_term], character_terms[-1])\n",
    "                    else:\n",
    "                        freq_term=most_frequent(merged_topics[edition][clean_term])\n",
    "                        \n",
    "                    #print(\">>>> ATENTION, merged_topics of %s are % and freq is %s\" %(clean_term, merged_topics[edition][clean_term], freq_term))\n",
    "                            \n",
    "                    freq_topics_terms[edition][clean_term]=freq_term\n",
    "                                        \n",
    "                    element[\"term\"]=freq_term.strip()\n",
    "                    \n",
    "                    \n",
    "                else:\n",
    "                    element[\"term\"]=clean_term.strip()\n",
    "        \n",
    "                    \n",
    "                if len(clean_term)>1:\n",
    "                    character_terms.append(clean_term[0])  \n",
    "\n",
    "            else:\n",
    "                #print(\">>> me salto %s - page %s\" %(element[\"term\"], current_page))\n",
    "                page_idx += 1\n",
    "                \n",
    "            \n",
    "        \n",
    "    new_results= delete_entries(query_results, eliminate_pages)\n",
    "    \n",
    "    return new_results, merged_topics, freq_topics_terms, provenance_removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_topics_refine(query_results):\n",
    "    \n",
    "    topics_editions={}\n",
    "    eliminate_pages={}\n",
    "    merged_topics_refine={}\n",
    "    provenance_removal={}\n",
    "    for edition in query_results:\n",
    "        eliminate_pages[edition]=[]\n",
    "        provenance_removal[edition]=[]\n",
    "        topics_editions[edition]={}\n",
    "        merged_topics_refine[edition]=[]\n",
    "        page_idx = 0\n",
    "        character=\"A\"\n",
    "        while page_idx < len(query_results[edition]):\n",
    "            \n",
    "            element = query_results[edition][page_idx][1]\n",
    "            term = element[\"term\"].strip()\n",
    "\n",
    "            m = re.search('^([0-9]+)|([IVXLCM]+)\\\\.?$', term)\n",
    "            \n",
    "            if \"Topic\" in element['type_page'] and term!=\"\" and len(term)>=3:\n",
    "            #and m is None:\n",
    "            #and term[0] >= character:\n",
    "                \n",
    "                if term not in topics_editions[edition]:\n",
    "                    topics_editions[edition][term]={}\n",
    "                    topics_editions[edition][term][\"start\"]=page_idx\n",
    "                    topics_editions[edition][term][\"end\"]= page_idx\n",
    "                    #print(\"NEW: Topic --%s-- - Start Page: %s, EN Page:%s \"%(term, topics_editions[edition][term][\"start\"], topics_editions[edition][term][\"end\"]))\n",
    "                else:\n",
    "                    topics_editions[edition][term][\"end\"]=page_idx\n",
    "                    #print(\"UPDATE: Topic --%s-- - Start Page: %s, EN Page:%s \"%(term, topics_editions[edition][term][\"start\"], topics_editions[edition][term][\"end\"]))        \n",
    "            \n",
    "            page_idx += 1\n",
    "            #if term:\n",
    "            #    character=term[0]\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        for term in topics_editions[edition]:\n",
    "            \n",
    "            p_start= topics_editions[edition][term][\"start\"]\n",
    "            p_end =  topics_editions[edition][term][\"end\"]\n",
    "            first_element= query_results[edition][p_start][1]\n",
    "            #print(\"NEW: Topic --%s-- - Start Page: %s, EN Page:%s \"%(term, query_results[edition][topics_editions[edition][term][\"start\"]][0], query_results[edition][topics_editions[edition][term][\"end\"]][0]))\n",
    "            \n",
    "            for p_id in range (p_start + 1, p_end+1):\n",
    "                element = query_results[edition][p_id][1]\n",
    "                first_element[\"definition\"]+=element[\"definition\"]\n",
    "                first_element[\"num_article_words\"]+=element[\"num_article_words\"]\n",
    "                first_element[\"num_page_words\"]+=element[\"num_page_words\"]                  \n",
    "                first_element[\"related_terms\"]+= element[\"related_terms\"]\n",
    "                first_element[\"end_page\"] = element['end_page']\n",
    "                provenance_removal[edition].append(first_element[\"end_page\"])\n",
    "                merged_topics_refine[edition].append(term)\n",
    "                eliminate_pages[edition].append(p_id)\n",
    "        \n",
    "    new_results= delete_entries(query_results, eliminate_pages)\n",
    "    \n",
    "    return new_results, provenance_removal, merged_topics_refine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Reading data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we are going to take the output of the defoe files, and we are going to merge the terms that splitted across pages. \n",
    "\n",
    "The next line takes time!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#query_results=read_query_results('toy_1771')\n",
    "query_results=read_query_results('results_eb_1_edition')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dc_results = copy.deepcopy(query_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It turns out that the results are order by the ALTO PAGE NUMB. And there are some ALTO PAGES NUM which have a higher value than the PAGE NUMBER. So, we have to sort the elements to make sure that we dont have inconsistencies. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_results= sort_query_results(dc_results)\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "consistency_query_results(sorted_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Merging articles splitted across pages\n",
    "\n",
    "Articles normaly have a lenght of one or two paragraphs, but they could be splitted across two consecutive pages.\n",
    "So, we are going to merge the detected splitted articles into one. \n",
    "\n",
    "**NEW** Furthemore, we are going to check the first 20 elements (pages), and check again if they should be FullPage or not based on their header and terms. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_results_articles_fp=fixing_fullPages(sorted_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_results_articles = merge_articles(query_results_articles_fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since some pages with articles could have wronlgy been classified as \"topics\", we are going to go through the \"TOPICS\" pages, and try to extract again articles from those again - reclassifying those as Articles. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles_refined=fixing_articles_1st(query_results_articles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Merging topics together across pages \n",
    "\n",
    "First, we are going to transfor those pages, which only have 1 article, and the previous term is a topic, into topics, and give the previous term to these ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles_refined= fixing_topics_1st(articles_refined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles_refined= removing_fullpages(articles_refined)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we are goint to merge topics.\n",
    "Topics can be across several pages, so we are going to **merge consecutive page-topics together**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics_refined, merged_topics, freq_topics_terms, provenance_removal =merge_topics(articles_refined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for ed in merged_topics:\n",
    "#    for clean_term in merged_topics[ed]:\n",
    "#        print(\"ED: %s, term %s, meged_topics %s\" %(ed, clean_term,  merged_topics[ed][clean_term]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for ed in freq_topics_terms:\n",
    "#    print(\"ED: %s, freq_topics_terms %s\" %(ed, freq_topics_terms[ed]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for ed in provenance_removal:\n",
    "#    print(\"ED: %s, provenance_removal %s\" %(ed, provenance_removal[ed]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1 Merging topics even more\n",
    "\n",
    "Before, we were merging topics that were in consecutive pages. But very often we have other types of pages (with tables, figures) between the same topic. Therefore we are going to merge the pages with the same topics. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_refine, provenance_removal_refine,merged_topics_refine =merge_topics_refine(topics_refined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for ed in merged_topics_refine:\n",
    "#    print(\"ED: %s, meged_topics %s\" %(ed, merged_topics_refine[ed]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for ed in provenance_removal_refine:\n",
    "#   print(\"ED: %s, provenance_removal_refine %s\" %(ed, provenance_removal_refine[ed]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the data has been merged, we are going to store it in a file, just to have the data merged.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#write_query_results(\"results_eb_1_edition_updated_new\", final_refine)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Creating a dataframe from the updated results\n",
    "\n",
    "Once, we have the terms properly merged, we are going to create a dataframe, which we will be use later to do further exploration. In this dataframe we have dropped some information from the original defoe files, that we don not longer need. \n",
    "\n",
    "**The dataframe will have the following columns**\n",
    "\n",
    "- definition:           Definition of a term\n",
    "- editionNum:           1,2,3,4,5,6,7,8\n",
    "- editionTitle:         Title of the edition\n",
    "- header:               Header of the page's term                                  \n",
    "- place:                Place where the volume was edited (e.g. Edinburgh)                                    \n",
    "- relatedTerms:         Related terms (see X article)  \n",
    "- altoXML:              File Path of the XML file from which the term belongs       \n",
    "- term:                 Term name                            \n",
    "- positionPage:         Position of ther term in the page     \n",
    "- startsAt:             Number page in which the term definition starts \n",
    "- endsAt:               Number page in which the term definition ends \n",
    "- volumeTitle:          Title of the Volume\n",
    "- typeTerm:             Type of term [Topic| Articles]                                       \n",
    "- year:                 Year of the edition\n",
    "- volumeNum:            Volume number (e.g. 1)\n",
    "- letters:              leters of the volume (A-B)\n",
    "- part:                 Part of the volume (e.g 1)\n",
    "- supplementTitle:      Supplement's Title\n",
    "- supplementsTo:        It suppelements to editions [1, 2, 3....]\n",
    "- numberOfWords:        Number of words per term definition\n",
    "- numberOfTerms:        Number of terms per page\n",
    "- numberOfPages:        Number of pages per volume\n",
    "- numberOfVolumes:      Number of volumes per edition or supplement\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IMPORTANT DECISIONs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Going to convert all the MIX articles (inside the dataframe)\n",
    "2. I am going to fiter OUT all the entries which are not Articles nor Topics. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df=create_dataframe_from_file(\"results_eb_1_edition_updated\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=create_dataframe(final_refine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "includeKeywords=[\"Article\", \"Topic\"]\n",
    "df=df[df[\"typeTerm\"].str.contains('|'.join(includeKeywords))].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_vol1=df[ (df[\"volumeNum\"]==1) & (df[\"year\"]==1771) ]\n",
    "df_vol1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_vol1.loc[18]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Saving the dataframe to json file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_json(r'../../results_NLS/results_eb_1_edition_dataframe_new', orient=\"index\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[1100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[1097]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[1098]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range (0, len(df.index)):\n",
    "    if i < len(df.index)-1:\n",
    "        end_year=df.loc[i, 'year']\n",
    "        end_vol=df.loc[i, 'volumeNum']\n",
    "        end=df.loc[i, 'endsAt']\n",
    "        \n",
    "        start=df.loc[i+1, 'startsAt']\n",
    "        start_year=df.loc[i+1, 'year']\n",
    "        start_vol=df.loc[i+1, 'volumeNum']\n",
    "        if start < end and end_year == start_year  and end_vol == start_vol:\n",
    "            print(\"INCONSISTENCY at rows %s and %s\" %(i, i+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df[\"term\"]==\"ABACUS\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[1062]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[1063]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['term'].str.contains(\"AGRICULTURE\")][\"typeTerm\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_1771_small = df_vol1[df_vol1['typeTerm']==\"Topic\"]\n",
    "df_1771_small = df_1771_small.head(100).reset_index(drop=True)\n",
    "df_1771_small"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
